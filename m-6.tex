\section{Lineaarikuvaukset} \label{lineaarikuvaukset}
\alku
\index{lineaarikuvaus|vahv}
\index{funktio A!g@lineaarikuvaus|vahv}

Avaruuksien $\R^n$ ja $\R^m$ v‰liseksi \kor{lineaarikuvaukseksi} sanotaan funktiota tyyppi‰
\[ 
\mv{f}:\ \R^n \kohti \R^m, \quad m,n \in \N, 
\]
($\DF_{\mv{f}}=\R^n,\ \RF_{\mv{f}}\subset\R^m$), jolle on voimassa ehto
\begin{equation} \label{lineaarikuvauksen ehto} 
\mv{f}(\alpha \mx + \beta \my)\ =\ \alpha \mv{f}(\mx) + \beta \mv{f}(\my), \quad 
                                         \mx,\my \in \R^n,\ \alpha,\beta \in \R.
\end{equation}
Lineaarikuvauksia tarkasteltaessa on tapana poiketa hieman tavallisista funktiomerkinnˆist‰: 
Lineaarikuvausta merkit‰‰n
\[ 
\mv{f}(\mx) = A \mx \quad \text{(lineaarikuvaus)}. 
\]
Oikealla siis j‰tet‰‰n sulkeet pois muuttujan ymp‰rilt‰, ja t‰m‰ ilmenee myˆs lukutavassa: 
Luetaan '$A\,x$' mieluummin kuin '$A$ $x$:ss‰'. Lukutavan mukaisesti ajatellaan kuvaamisen 
(eli funktion arvon m‰‰r‰‰misen) tapahtuvan siten, ett‰ $A$ \kor{operoi} $\mx$:‰‰n. 
Operoinnin voi ymm‰rt‰‰ abstraktina kertolaskuna (= matriisin ja vektorin tulo), joka on
vektorien yhteenlaskun ett‰ skalaarilla kertomisen suhteen \kor{lineaarinen} eli toteutaa
tavanomaiset osittelulait n‰iden laskuoperaatioiden suhteen:
\index{lineaarisuus!c@lineaarikuvauksen}
\[ 
\boxed{ \quad\kehys A(\alpha \mx + \beta \my) 
            = \alpha A \mx + \beta A \my \quad \text{(lineaarikuvaus)}. \quad } 
\]
Lineaarikuvausten symboleina k‰ytet‰‰n yleens‰ isoja kirjaimia $A,B$ jne. 

Jos \mA\ on matriisi kokoa $m \times n$, niin matriisialgebran s‰‰ntˆjen perusteella on 
ilmeist‰, ett‰ funktio $\mv{f}(\mx) = \mA \mx$ on lineaarikuvaus tyyppi‰ 
$\mv{f}:\ \R^n \kohti \R^m$. Osoittautuu, ettei muunlaisia, t‰t‰ tyyppi‰ olevia
lineaarikuvauksia olekaan:
\begin{Prop} \label{lineaarikuvaukset ja matriisit} Jos $\mv{f}:\ \R^n \kohti \R^m$ on 
lineaarikuvaus, niin on olemassa yksik‰sitteinen matriisi \mA\ kokoa $m \times n$, siten ett‰
$\mv{f}(\mx) = \mA\mx,\ \mx \in \R^n$. \end{Prop}
\tod Kun $\mx \in \R^n$ esitet‰‰n kannan $\{\me_1, \ldots, \me_n\}$ avulla muodossa
\[ 
\mx\ =\ \sum_{i=1}^n x_i \me_i, 
\]
niin $\mv{f}$:n lineaarisuuden ja matriisialgebran s‰‰ntˆjen (Luku \ref{matriisialgebra}) 
nojalla
\[ 
\mv{f}(\mx)\ =\ \sum_{i=1}^n x_i \mv{f}(\me_i)\ =\ \mA\mx, \qquad 
                             \mA=[\mv{f}(\me_1) \ldots \mv{f}(\me_n)]. \loppu
\]

Proposition \ref{lineaarikuvaukset ja matriisit} perusteella lineaarikuvausten ja matriisien 
v‰lill‰ on k‰‰nt‰en yksik‰sitteinen vastaavuus:
\[ 
\text{lineaarikuvaus}\ A:\ \R^n \kohti \R^m \ \ \longleftrightarrow\ \ 
            \text{matriisi}\ \mA\ \text{kokoa}\ m \times n\ \ (A\mx = \mA\mx).
\]
Jos matriisi \mA\ vastaa lineaarikuvausta $A$, niin vastaavuuteen viitataan sanomalla, ett‰ 
\index{lineaarikuvaus!lineaarikuvauksen matriisi}%
$\mA$ on ko.\ \kor{lineaarikuvauksen matriisi} (tai esitysmatriisi). Esitysmatriisin kautta 
voidaan siis lineaarikuvauksilla laskeminen aina palauttaa matriisialgebraan. Toisaalta voidaan
matriisin ja vektorin kertolasku aina haluttaessa n‰hd‰ lineaarisena kuvauksena, ts.\ er‰‰n 
lineaarikuvauksen laskus‰‰nnˆn soveltamisena. Erityisesti kahden matriisin tulo vastaa kahden
lineaarikuvauksen yhdistely‰:
\[ 
AB\ \vast\ \mA\mB. 
\]
Yhdistely‰ koskevat rajoitukset ovat samat kuin matriisitulossa: On oletettava, ett‰ 
$B:\ \R^n \kohti \R^p$ ja  $A:\ \R^p \kohti \R^m$ joillakin $m,n,p\in\N$, jolloin myˆs tulo 
$\mA\mB$ on m‰‰ritelty. 

\index{kzyzy@k‰‰nteiskuvaus}%
Lineaarikuvauksen $A:\ \R^n \kohti \R^m$ \kor{k‰‰nteiskuvauksen} $A^{-1}$ muodollinen
m‰‰ritelm‰ on
\[ 
A\mx = \my \quad \ekv \quad \mx = A^{-1}\my. 
\]
K‰‰nteiskuvaus on t‰ll‰ tavoin m‰‰ritelt‰viss‰, mik‰li $A$ on 1-1 ('yksi yhteen'), eli toteuttaa
ehdon
\[ 
A\mx_1 = A\mx_2 \quad \impl \quad \mx_1 = \mx_2. 
\]
Koska $A$ on lineaarinen, t‰m‰ on sama kuin ehto
\[ 
A\mx = \mv{0} \quad \impl \quad \mx = \mv{0}. 
\] 
Aiemmista tuloksista (Luku \ref{tuettu Gauss}) voidaan p‰‰tell‰, ett‰ t‰m‰ ehto voi toteutua 
vain jos $m \ge n$ (v‰ltt‰m‰tˆn ehto). Jos ehto toteutuu, on $A$ \kor{k‰‰ntyv‰}, ja 
k‰‰nteiskuvaus on t‰llˆin m‰‰ritelty kuvauksena tyyppi‰ $A^{-1}:\ \R^m \kohti \R^n$. 
M‰‰rittelyjoukko ei kuitenkaan ole v‰ltt‰m‰tt‰ koko $\R^m$, sill‰ k‰‰nteiskuvauksen m‰‰ritelm‰n
mukaan $A^{-1}\my$ on m‰‰ritelty vain sellaisille vektoreille $\my \in \R^m$, joille 
$\my = \mA\mx$ jollakin $\mx \in \R^n$, ts.\ yht‰lˆryhm‰ll‰ $\mA\mx = \my$ on oltava ratkaisu. 
Aiempien tulosten perustella yht‰lˆryhm‰n $\mA\mx = \my$ ratkeavuus jokaisella $\my \in \R^m$ on
mahdollinen vain jos $m \le n$ (v‰ltt‰m‰tˆn ehto). P‰‰tell‰‰n siis, ett‰ k‰‰nteiskuvaus $A^{-1}$
on on olemassa ja m‰‰ritelty koko $\R^m$:ss‰ t‰sm‰lleen kun $m=n$ ja $A$:n matriisi $\mA$ 
(neliˆmatriisi) on s‰‰nnˆllinen. K‰‰nteiskuvaus on t‰llˆin lineaarikuvaus tyyppi‰ 
$A^{-1}:\,\R^n \kohti \R^n$ (samaa tyyppi‰ kuin $A$), ja k‰‰nteiskuvauksen matriisi on 
luonnollisesti $\mA^{-1}$.

Lineaarikuvauksen $A:\ \R^n \kohti \R^n$ k‰‰nteiskuvauksen lineaarisuutta k‰ytet‰‰n usein
hyˆdyksi lineaarisia yht‰lˆryhmi‰ ratkaistaessa. Tyypillinen sovellutustilanne on sellainen,
ett‰ yht‰lˆryhm‰ $\mA\mx = \mb$ halutaan ratkaista useilla eri $\mb$:n arvoilla \mA:n pysyess‰ 
samana. Jos ratkaisua merkit‰‰n kuvauksena $\mb \map \mx$ (kuvauksen $\mx\map\mb=\mA\mx$ 
k‰‰nteiskuvaus) ja oletetaan, ett‰ tunnetaan ratkaisut $\mb_1 \map \mx_1$ ja 
$\mb_2 \map \mx_2$, niin lineaarisuuden perusteella tiedet‰‰n 
(vrt.\ Luku \ref{Gaussin algoritmi}, Esimerkki \ref{ristikkoesimerkki})
\[ 
\alpha \mb_1 + \beta \mb_2\ \longmapsto\ \alpha \mx_1 + \beta \mx_2 \quad (\alpha,\beta \in \R). 
\]

\subsection*{Kannan vaihto $\R^n$:ss‰}
\index{kanta!b@kannan vaihto|vahv}

$\R^n$:n vektorisysteemi‰ (j‰rjestetty‰ joukkoa) $\{\ma_1, \ldots \ma_m\}$ sanotaan 
\index{lineaarinen riippumattomuus}%
\kor{lineaarisesti riippumattomaksi}, jos p‰tee
\[ 
x_1 \ma_1 + \ldots + x_m \ma_m\ =\ \mv{0} \quad \impl \quad x_i = 0, \ \ i = 1 \ldots m. 
\]
Jos vektorit $\ma_i$ kootaan matriisin $\mA$ sarakkeiksi ($\mA$ kokoa $n \times m$), niin 
voidaan kirjoittaa $x_1 \ma_1 + \ldots + x_m \ma_m\ = \mA\mx$ 
(vrt.\ Luku \ref{matriisialgebra}), joten lineaarisen riippumattomuuden ehto saa muodon
\[ 
\mx \in \R^m\ \ja\ \mA\mx = \mv{0} \quad \impl \quad \mx = \mv{0}, \quad\quad \mA 
                          = [\ma_1, \ldots, \ma_m]. 
\]
Luvun \ref{tuettu Gauss} tulosten mukaan t‰m‰ ehto voi olla voimassa vain kun $m \le n$. Siis
$\R^n$:ss‰ on jokainen $n+1$ vektorin (tai useamman vektorin) muodostama systeemi 
\index{lineaarinen riippuvuus}%
\kor{lineaarisesti riippuva}. Toisaalta jos $m=n$, niin lineaarisesti riippumattoman systeemin
muodostaa ainakin $\R^n$:n luonnollinen kanta $\{\me_1, \ldots \me_n\}$, jota ym.\ ehdossa 
vastaa matriisi $\mA=\mI$. Luku $n$ on siis lineaarisen riippumattomuuden kannalta kriittinen
luku, ja t‰m‰ onkin jo aiemmin nimetty avaruuden $\R^n$
\index{dimensio}%
\kor{dimensioksi} (=ulotteisuus). Jos
$m=n$, niin ym.\ ehto toteutuu t‰sm‰lleen kun $\mA$ on s‰‰nnˆllinen matriisi 
(Lause \ref{s‰‰nnˆllisyyskriteerit}). N‰in ollen, ja koska $\mA$ on s‰‰nnˆllinen t‰sm‰lleen kun
$\mA^T$ on s‰‰nnˆllinen (ks.\ Luku \ref{inverssi}), saadaan neliˆmatriisin s‰‰nnˆllisyydelle
seuraavat uudet (geometriset) tulkinnat:
\[ \boxed{ \begin{aligned} 
\ykehys\quad \mA\ \text{s‰‰nnˆllinen} \quad 
             &\ekv \quad \text{\mA:n sarakkeet lineaarisesti riippumattomat} \quad \\
\akehys      &\ekv \quad \text{\mA:n rivit lineaarisesti riippumattomat}. \quad
           \end{aligned} } \] 
Jos $\{\ma_1, \ldots, \ma_n\}$ on $\R^n$:n lineaarisesti riippumaton vektorisysteemi, eli 
matriisi $\mA = [\ma_1, \ldots, \ma_n]$ on s‰‰nnˆllinen, niin mielivaltainen vektori 
$\mb \in \R^n$ voidaan lausua yksik‰sitteisesti vektorien $\ma_i$ lineaarisena yhdistelyn‰. 
Nimitt‰in kertoimet $x_i$ ratkeavat yksik‰sitteisesti yht‰lˆryhm‰st‰
\[ 
\sum_{i=1}^n x_i \ma_i = \mb \quad \ekv \quad \mA\mx = \mb.
\]
T‰ll‰ perusteella voidaan sanoa, ett‰ jokainen lineaarisesti riippumaton $n$ vektorin systeemi
$\{\ma_1, \ldots, \ma_n\}$ on $\R^n$:n
\index{kanta}%
\kor{kanta}. Jos siis \mA\ on mik‰ tahansa s‰‰nnˆllinen
matriisi kokoa $n \times n$, niin $\mA$:n sarakkeet (myˆs rivit) muodostavat $\R^n$:n kannan. 
--- Huomattakoon, ett‰ t‰llainen kanta on
\index{kanta!a@ortonormeerattu}%
\kor{ortonormeerattu}, eli 
$\scp{\ma_i}{\ma_j} = \delta_{ij}$ (Kroneckerin $\delta$), t‰sm‰lleen kun \mA\ on ortogonaalinen
(vrt.\ Luku \ref{inverssi}). P‰‰tell‰‰n siis, ett‰ jokainen s‰‰nnˆllinen matriisi kokoa 
$n \times n$ edustaa sarakkeittensa (tai riviens‰) kautta $\R^n$:n kantaa, ja jokainen 
ortogonaalinen matriisi edustaa ortonormeerattua kantaa.

Jos halutaan suorittaa \kor{kannan vaihto}, l‰htˆkohtana $\R^n$:n luonnollinen kanta 
$\{\me_1, \ldots, \me_n\}$, niin vektorin $\mx$ koordinaatit $x'_i$ kannassa 
$\{\ma_1, \ldots \ma_m\}$ ovat siis ratkaistavissa yht‰lˆryhm‰st‰
\[ 
\mA\mx' = \mx \quad \impl \quad \mx' = \mA^{-1}\mx, \quad\quad \mA = [\ma_1, \ldots, \ma_n]. 
\]
Kannan vaihdossa, eli koordinaattimuunnoksessa $\mx \map \mx'$, on siis kyse 
lineaarikuvauksesta. Jos uusikin kanta on ortonormeerattu, niin muunnoksen matriisi on helppo
laskea: $\mA^{-1}=\mA^T$.
\begin{Exa} M‰‰rit‰ vektorin $x \vec{i} + y \vec{j} + z \vec{k}$ koordinaatit $x',y',z'$ 
kannassa, jonka muodostavat vektorit $\vec{a}_1 = \vec{i} + \vec{j}$, 
$\ \vec{a}_2 = \vec{j} + \vec{k}$ ja $\vec{a}_3 = \vec{i} + \vec{k}$.
\end{Exa}
\ratk 
\[ 
\begin{rmatrix} 1&0&1\\1&1&0\\0&1&1 \end {rmatrix} \begin{rmatrix} x'\\y'\\z' \end{rmatrix} =
\begin{rmatrix} x\\y\\x \end{rmatrix} \quad \impl \quad 
\begin{rmatrix} x'\\y'\\z' \end{rmatrix} =
     \frac{1}{2} \begin{rmatrix} 1&1&-1\\-1&1&1\\1&-1&1 \end{rmatrix} 
                 \begin{rmatrix} x\\y\\z \end{rmatrix}. \loppu
\]
\begin{Exa} \label{pallokoordinaatisto ja matriisit} 
Pallokoordinaatiston pisteess‰ $P=(r,\theta,\varphi)$ k‰ytet‰‰n avaruusvektoreille
kantaa $\{\vec e_r,\vec e_\theta,\vec e_\varphi\}$, miss‰ (vrt.\ Luku \ref{koordinaatistot})
\[
\left\{\begin{array}{ll}
\vec e_r       &= \ \sin{\theta} \cos{\varphi} \vec i + \sin{\theta}\sin{\varphi} \vec j 
                                                      + \cos{\theta} \vec k, \quad \\[4pt]
\vec e_\theta  &= \ \cos{\theta} \cos{\varphi} \vec i + \cos{\theta}\sin{\varphi} \vec j 
                                                      - \sin{\theta} \vec k, \\[4pt]
\vec e_\varphi &= \ -\sin{\varphi} \vec i + \cos{\varphi} \vec j.
\end{array} \right.
\]
Kun kirjoitetaan
\[
\vec F \,=\, F_1\vec i+F_2\vec j+F_3\vec k
       \,=\, F_r\vec e_r+F_\theta\vec e_\theta+F_\varphi\vec e_\varphi\,,
\]
niin muunnoskaava $(F_r,F_\theta,F_\varphi) \map (F_1,F_2,F_3)$ on matriisimuodossa
\[
\begin{rmatrix}
\sin\theta\cos\varphi&\cos\theta\cos\varphi&-\sin\varphi\\
\sin\theta\sin\varphi&\cos\theta\sin\varphi&\cos\varphi \\
\cos\theta&-\sin\theta&0
\end{rmatrix}
\begin{bmatrix} F_r\\F_\theta\\F_\varphi \end{bmatrix} =
\begin{bmatrix} F_1\\F_2\\F_3 \end{bmatrix}.
\]
Koska $\{\vec e_r,\vec e_\theta,\vec e_\varphi\}$ on ortonormeerattu systeemi, niin
k‰‰nteismuunnos saadaan transponoimalla:
\[
\begin{bmatrix} F_r\\F_\theta\\F_\varphi \end{bmatrix} =
\begin{rmatrix}
\sin\theta\cos\varphi&\sin\theta\sin\varphi&\cos\theta\\
\cos\theta\cos\varphi&\cos\theta\sin\varphi&-\sin\theta \\
-\sin\varphi&\cos\varphi&0
\end{rmatrix}
\begin{bmatrix} F_1\\F_2\\F_3 \end{bmatrix}. \loppu
\]
\end{Exa}

\subsection*{Koordinaatiston kierto}
\index{koordinaatisto!d@koordinaatiston kierto|vahv}
\index{kierto!b@koordinaatiston|vahv}

Edell‰ todettiin, ett‰ jos $\mA$ on ortogonaalinen matriisi kokoa $n \times n$, niin $\mA$:n
sarakkeet (myˆs rivit) m‰‰rittelev‰t $\R^n$:n ortonormeeratun kannan. Sanotaan, ett‰ ko.\
\index{suunnistus (kannan)} \index{positiivinen suunnistus!a@kannan} 
\index{negatiivisesti!a@suunnistettu (kanta)}%
kanta on \kor{positiivisesti suunnistettu} (tai 'oikeak‰tinen'), jos $\det\mA=1$ ja
\kor{negatiivisesti suunnistettu} ('vasenk‰tinen'), jos $\det\mA=-1$. (Muita mahdollisia
arvoja ei $\det\mA$:lla ole, ks. Harj.teht. 
\ref{determinantti}:\,\ref{H-m-5: ortogonaalisen matriisin determinantti}). Jos yhden 
kantavektorin suunta vaihdetaan, niin determinanttiopin mukaisesti $\det\mA$:n merkki vaihtuu,
eli kannan suunnistus vaihtuu.
\begin{Exa} Jos $\{\vec a,\vec b,\vec c\,\}$ on avaruusvektoreiden ortonormeerattu kanta, niin
ko.\ vektoreiden koordinaattivektorit kannassa $\{\vec i,\vec j,\vec k\,\}$ muodostavat
$\R^3$:n ortonormeeratun kannan $\{\ma,\mb,\mc\}$, eli matriisi $\mA=[\ma\,\mb\,\mc]$ on
ortogonaalinen (kuten edellisess‰ esimerkiss‰). Geometrisin vektorioperaatioin laskettuna
$\mA$:n determinantti on (vrt.\ Luku \ref{ristitulo})
\[
\det\mA=\det\mA^T=(\vec a\times\vec b)\cdot\vec c=[\,\vec a\,\vec b\,\vec c\,].
\]
Koska $\{\vec a,\vec b,\vec c\,\}$ on ortonormeerattu systeemi, niin on joko 
$\vec c=\vec a\times\vec b$, jolloin $\det\mA=\abs{\vec c\,}^2=1$, tai 
$\vec c=-(\vec a\times\vec b)$, jolloin $\det\mA=-1$. Edellisess‰ tapauksessa kanta on siis 
positiivisesti, j‰lkimm‰isess‰ negatiivisesti suunnistettu. \loppu
\end{Exa}
\begin{Exa} \label{napakoordinaatisto ja matriisit} Napakoordinaatiston kannat 
$\{\vec e_r,\vec e_\varphi\}$ (vrt.\ Luku \ref{koordinaatistot}) kattavat eri $\varphi$:n 
arvoilla kaikki mahdolliset tason vektoreiden ortonormeeratut, positiivisesti suunnistetut
systeemit. N‰in ollen kaikki ortogonaaliset matriisit $\mA$ kokoa $2 \times 2$, jotka t‰ytt‰v‰t
ehdon $\det\mA=1$, ovat muotoa
\[
\mA = \begin{rmatrix} 
      \cos\varphi&\sin\varphi\\-\sin\varphi&\cos\varphi
      \end{rmatrix}, \quad \varphi\in[0,2\pi). \loppu
\]
\end{Exa}

Jos $\mA=[\ma_1 \ldots \ma_n]$ on ortogonaalinen matriisi ja $\det\mA=1$, niin kannan vaihtoa
$\{\me_1,\ldots,\me_n\} \ext \{\ma_1,\ldots,\ma_n\}$ sanotaan \kor{koordinaatiston kierroksi}.
Nimitt‰in on osoitettavissa, ett‰ kannan vaihto on t‰llˆin toteutettavissa j\pain{atkuvana} 
muunnoksena muotoa
\[
t\in[0,1]\ \map\ \{\ma_1(t),\ldots\,\ma_n(t)\}
\]
siten, ett‰ seuraavat ehdot toteutuvat:
\begin{itemize}
\item[(i)]   $\ma_i(0)=\me_i$ ja $\ma_i(1)=\ma_i$, $\ i=1 \ldots n$.
\item[(ii)]  Vektorisysteemi $\{\ma_1(t),\ldots,\ma_n(t)\}$ on ortonormeerattu ja positiivisesti
             suunnistettu jokaisella $t\in[0,1]$.
\item[(iii)] Jatkuvuusehto: Funktiot $t\in[0,1]\map\ma_j(t)\in\R^n$ ovat jatkuvia, ts.\
             reaalifunktiot $t \map a_{ij}(t)=[\ma_j(t)]_i,\ i,j=1 \ldots n\,$ ovat jatkuvia 
             v‰lill‰ $[0,1]$. 
\end{itemize}
Ehdot t‰ytt‰v‰ muunnos voidaan kuvitella toteutettavan niin, ett‰ vektorisysteemi‰
$\{\me_1,\ldots,\me_n\}$ kierret‰‰n 'j‰ykk‰n‰ kappaleena', jolloin ortogonaalisuus ja suunnistus
s‰ilyv‰t. Perussysteemist‰ $\{\me_1,\ldots,\me_n\}$ l‰htien on siis t‰llaisella jatkuvalla 
kierrolla mahdollista p‰‰ty‰ mihin tahansa annettuun, ortonormeerattuun ja positiivisesti 
suunnistettuun systeemiin $\{\ma_1,\ldots,\ma_n\}$. Jos ko.\ systeemi on negatiivisesti 
suunnistettu, niin jossakin on tapahduttava 'hyppy' (yhden vektorin suunnan vaihto), jolloin 
muunnos ei ole jatkuva. T‰m‰n v‰itt‰m‰n mukaisesti siis kaikki $\R^n$:n positiivisesti 
suunnistetut ortonormeeratut systeemit ovat kesken‰‰n (samoin negatiivisesti suunnistetut 
\index{kiertoekvivalenssi}%
kesken‰‰n) \kor{kiertoekvivalentteja}. Tapauksessa $n=2$ v‰itt‰m‰n voi todistaa
Esimerkin \ref{napakoordinaatisto ja matriisit} tuloksen perusteella 
(Harj.teht.\,\ref{H-m-6: jatkuva kierto tasossa}). Yleinen tapaus on haastavampi --- tarkemmat
perustelut sivuutetaan.

\subsection*{$\R^n$:n aliavaruudet}
\index{aliavaruus|vahv}

Olkoon annettu $m$ vektoria $\ma_j\in\R^n,\ j=1 \ldots m$ (voi olla $m \le n$ tai $m>n$).
T‰llˆin vektorijoukko, joka m‰‰ritell‰‰n
\[
W = \{\mpv\in\R^n \mid \mpv=\sum_{j=1}^m y_j\ma_j\,\ 
                                   \text{jollakin}\,\ [y_1,\ldots,y_m]^T\in\R^m\}
\]
on $\R^n$:n \kor{aliavaruus}, sill‰ m‰‰ritelyn perusteella p‰tee $W\subset\R^n$ ja
\[
\mpv_1,\mpv_2 \in W \qimpl \alpha_1\mpv_1+\alpha_2\mpv_2 \in W \quad 
                                          \forall\ \alpha_1,\alpha_2\in\R.
\]
\index{viritt‰‰ (aliavaruus)}%
Sanotaan, ett‰ $W$ on vektoreiden $\ma_1,\ldots,\ma_m$ \kor{viritt‰m‰} avaruus. Jos vektorit
$\ma_i$ ovat lineaarisesti riippumattomat (jolloin on oltava $m \le n$), niin dim W $=m$ ja 
$\{\ma_1,\ldots\ma_m\}$ on $W$:n kanta. Ovatko vektorit $\ma_i$ lineaarisesti riippumattomat vai
eiv‰t, selvi‰‰ Gaussin algoritmilla: Muodostetaan vektorit $\ma_j$ sarakkeina matriisi 
$\mA=[\ma_1\,\ldots\,\ma_m]$ ja etsit‰‰n yht‰lˆryhm‰n $\mA\mx=\mo$ kaikki ratkaisut $\mx\in\R^m$
(vrt.\ Luku \ref{tuettu Gauss}).
\begin{Exa} Ovatko $\R^4$:n vektorit $\ma_1 = [1,2,1,-1]^T$, $\ma_2 = [2,-1,3,-1]^T$ ja 
$\ma_3 = [0,5,-1,-1]^T$ lineaarisesti riippumattomat? 
\end{Exa}
\ratk Muunnetaan yht‰lˆryhm‰ $\mA\mx = \mv{0}$ Gaussin algoritmilla singulaariseen perusmuotoon
$\mU\mx = \mv{0}$:
\[ 
\begin{rmatrix} 1&2&0\\2&-1&5\\1&3&-1\\-1&-1&-1 \end{rmatrix} 
\begin{rmatrix} x_1\\x_2\\x_3 \end{rmatrix}\ =\ 
\begin{rmatrix} 0\\0\\0\\0 \end{rmatrix} \quad \ekv \quad
\begin{rmatrix} 1&2&0\\0&-5&-5\\0&0&0\\0&0&0 \end{rmatrix} 
\begin{rmatrix} x_1\\x_2\\x_3 \end{rmatrix}\ =\ \begin{rmatrix} 0\\0\\0\\0 \end{rmatrix}. 
\]
T‰m‰n perusteella
\[ 
\mA\mx = \mv{0} \quad \ekv \quad \mx = t\,[-2,1,1]^T,\ t \in \R. 
\] 
Vastaus: Eiv‰t, sill‰ $-2\ma_1+\ma_2+\ma_3 = \mv{0}$. \loppu 

Gaussin algoritmilla selvi‰‰ myˆs yleisemmin, mik‰ on annettujen vektorien
$\ma_j\in\R^n,\ j=1 \ldots m$ viritt‰m‰n aliavaruuden $W$ dimensio ja mik‰ ko.\
vektorisysteemin lineaarisesti riippumaton \pain{osas}y\pain{steemi} kelpaa $W$:n kannaksi 
silloin, kun koko systeemi on lineaarisesti riippuva.
\jatko \begin{Exa} (jatko) Esimerkin matriisin $\mA$ sarakkeet viritt‰v‰t $\R^4$:n aliavaruuden
$W$ ja rivit $\R^3$:n aliavaruuden $U$. M‰‰rit‰ n‰iden aliavaruuksien dimensiot ja kummallekin 
jokin kanta.
\end{Exa}
\ratk
Esimerkin algoritmissa ei tehty rivien eik‰ sarakkeiden vaihtoja, joten tuloksesta on suoraan 
luettavissa: dim $W=$ dim $U=2$, $W$:n er‰s kanta $=\mA$:n sarakkeet $1$ ja $2$ ja $U$:n er‰s 
kanta $=\mA$:n rivit $1$ ja $2$, ts.\
\begin{align*}
W &= \{\mpv=\alpha_1[1,2,1,-1]^T+\alpha_2[2,-1,3,-1]^T,\,\ \alpha_1,\alpha_2\in\R\}, \\
U &= \{\mpv=\alpha_1[1,2,0]+\alpha_2[2,-1,5],\,\ \alpha_1,\alpha_2\in\R\}. \loppu
\end{align*}
\begin{Exa} Matriisin
\[
\mA=\begin{rmatrix}
    3&3&0&3&4&1\\1&-3&2&-1&-1&2\\-3&3&-3&0&2&-1\\6&-2&4&2&1&3
    \end{rmatrix}
\]
sarakkeet viritt‰v‰t $\R^4$:n aliavaruuden $W$ ja rivit $\R^6$:n aliavaruuden $U$. M‰‰rit‰
n‰iden aliavaruuksien dimensiot ja kummallekin jokin kanta.
\end{Exa}
\ratk Yht‰lˆryhm‰n
\[
\begin{rmatrix}
3&3&0&3&4&1\\1&-3&2&-1&-1&2\\-3&3&-3&0&2&-1\\6&-2&4&2&1&3
\end{rmatrix}
\begin{bmatrix} x_1\\x_2\\x_3\\x_4\\x_5\\x_6 \end{bmatrix} =
\begin{bmatrix} 0\\0\\0\\0\\0\\0 \end{bmatrix}
\]
singulaariseksi perusmuodoksi saadaan tuetulla Gaussin algoritmilla
\[
\begin{rmatrix}
3&3&4&1&0&3 \\[1mm] 0&-4&-\frac{7}{3}&\frac{5}{3}&2&-2 \\[1mm]
0&0&\frac{5}{2}&\frac{5}{2}&0&0 \\[1mm] 0&0&0&0&0&0
\end{rmatrix} 
\begin{bmatrix} x_1\\x_2\\x_5\\x_6\\x_3\\x_4 \end{bmatrix} =
\begin{bmatrix} 0\\0\\0\\0\\0\\0 \end{bmatrix}.
\]
T‰h‰n muotoon p‰‰semiseksi ei ole tehty rivinvaihtoja. Sarakkeita sen sijaan on vaihdettu niin,
ett‰ uusi j‰rjestys on $125634$, kuten n‰kyy lopputuloksesta. P‰‰tell‰‰n siis: 
dim $W=$ dim $U=3$, $U$:n er‰‰n kannan muodostavat $\mA$:n kolme ensimm‰ist‰ rivi‰ ja $W$:n 
er‰‰n kannan $\mA$:n sarakkeet $1$, $2$ ja $5$. \loppu  

Em.\ esimerkeiss‰ matriisin sarakkeiden ja rivien viritt‰mien avaruuksien dimensiot ovat samat.
Kyse ei ole sattumasta, vaan dimensiot ovat \pain{aina} samat: dimensioiden yhteinen arvo = 
matriisin s‰‰nnˆllisyysaste (ks.\ Luku \ref{tuettu Gauss}).
 
\subsection*{*Ortogonaaliprojektio $\R^n$:ss‰}
\index{ortogonaaliprojektio|vahv}

Olkoon $\{\ma_1, \ldots, \ma_m\},\ 1 \le m < n$, lineaarisesti riippumaton vektorisysteemi 
$\R^n$:ss‰, ja olkoon $W$ ko.\ vektorien viritt‰m‰ $R^n$:n (aito) aliavaruus:
\[ 
W = \{\mv{v} \in \R^n \mid \mv{v} = \sum_{i=1}^m x_i \ma_i,\ \ x_i \in \R \}. 
\]
T‰llˆin jos $\mv{u} \in \R^n$, niin $\mv{u}$:n \kor{ortogonaaliprojektio} aliavaruuteen $W$ 
m‰‰ritell‰‰n vektorina $\mw$, joka toteuttaa
\[ 
\mw \in W\ \ \ja\ \ \scp{\mpu-\mw}{\mpv} = 0\ \ \forall\ \mpv \in W. 
\]
Kirjoittamalla t‰ss‰ $\mpv$ vektoreiden $\ma_1, \ldots, \ma_m$ lineaarikombinaationa 
(mahdollista jokaiselle $\mpv \in W$) ja k‰ytt‰m‰ll‰ skalaaritulon bilineaarisuutta n‰hd‰‰n,
ett‰ ortogonaalisuusehto riitt‰‰ asettaa vektoreille $\mpv=\ma_i,\ i= 1\,\ldots\,m\,$:
\[ 
\scp{\mpu-\mw}{\mpv} = 0\ \ \forall\ \mpv \in W  
               \qekv \scp{\mpu-\mw}{\ma_i} = 0,\ \ i = 1 \ldots m. 
\]
Ortogonaaliprojektion haku on n‰in pelkistetty yht‰lˆryhm‰ksi. Kun t‰ss‰ kirjoitetaan viel‰
$\mw = \sum_{j=1}^m x_j \ma_j$ (mahdollista, koska $\mw \in W$), ja k‰ytet‰‰n edelleen 
skalaaritulon bilineaarisuutta ja symmetrisyytt‰, niin n‰hd‰‰n, ett‰ kyseess‰ on seuraava 
lineaarinen yht‰lˆryhm‰ kerroinvektorille $\mx = [x_1, \ldots, x_m]^T\in\R^m$\,:
\[ 
\boxed{\quad\kehys \mA\mx = \mb, \qquad 
              [\mA]_{ij} = \scp{\ma_i}{\ma_j}, \quad [\mb]_i = \scp{\mpu}{\ma_i}, 
                                               \quad i,j = 1 \ldots m. \quad } 
\]
\index{normaalimuoto!b@projektio-ongelman}%
T‰t‰ muotoilua sanotaan projektio-ongelman \kor{normaalimuodoksi}. 

Projektio-ongelmaan p‰‰dyt‰‰n lineaarialgebran sovelluksissa usein minimointiongelman kautta:
Halutaan etsi‰ annettua vektoria $\mpu\in\R^n$ jonkin \pain{normin} $\norm{\cdot}$ mieless‰ 
\pain{l‰hin} vektori aliavaruudesta $W$, ts.\ halutaan ratkaista minimointiongelma
\[ 
\quad \mpv \in W: \quad \norm{\mpu-\mpv} = \text{min!} 
\]
Jos normi liittyy skalaarituloon $\,\scp{\cdot}{\cdot}\,$ siten, ett‰ 
$\norm{\mx}^2=\scp{\mx}{\mx}$ (ks.\ Luku \ref{abstrakti skalaaritulo}), niin ongelman ratkaisu
saadaan $\mpu$:n ortogonaaliprojektiona $\mw \in W$ ko.\ skalaaritulon suhteen. Nimitt‰in jos
$\mpv \in W$ on mielivaltainen ja merkit‰‰n $\mpv_0 = \mw-\mpv$, niin
$\scp{\mpu-\mw}{\mpv_0}=0$ (koska $\mpv_0 \in W$), joten seuraa
\begin{align*}
\norm{\mpu-\mpv}^2 &=\ \norm{(\mpu-\mw)+\mpv_0)}^2\ 
                    =\ \scp{\,(\mpu-\mw)+\mpv_0\,}{\,(\mpu-\mw)+\mpv_0\,} \\
                   &=\ \abs{\mpu-\mw}^2 + 2\,\scp{\mpu-\mw}{\mpv_0} + \abs{\mpv_0}^2 \\
                   &=\ \abs{\mpu-\mw}^2 + \abs{\mpv_0}^2.
\end{align*}
Siis $\norm{\mpu-\mpv}$:n minimiarvo saavutetaan, kun $\mpv_0=\mv{0}\ \ekv\ \mpv = \mw$.

Projektio-ongelman yksik‰sitteisen ratkeavuuden takaa
\begin{Lause} \label{projektiolause} (\vahv{$\R^n$:n projektiolause}) Jos $W \subset \R^n$ on
lineaarisesti riippumattomien vektorien $\ma_1, \ldots, \ma_m$ viritt‰m‰ $\R^n$:n aliavaruus 
($m<n$) ja $\mpu \in \R^n$, niin on olemassa yksik‰sitteinen $\mw \in W$ siten, ett‰ 
$\ \scp{\mpu-\mw}{\mpv} = 0\ \ \forall\ \mpv \in W$. 
\end{Lause}
\tod Lause v‰itt‰‰, ett‰ matriisi $\mA\ = (\scp{\ma_i}{\ma_j})$ 
(neliˆmatriisi kokoa $m \times m$) on s‰‰nnˆllinen. T‰m‰n n‰ytt‰miseksi olkoon
$\mw=\sum_{i=1}^m x_i\ma_i$ ja lasketaan ensin
\[ 
\abs{\mw}^2\  =\ \scp{\mw}{\mw}\ =\ \scp{\,\sum_{i=1}^m x_i \ma_i}{\sum_{j=1}^m x_j \ma_j\,}. 
\]
Skalaaritulon bilineaarisuuden ja matriisialgebran nojalla t‰m‰ purkautuu muotoon
\[ 
\abs{\mw}^2\ =\ \sum_{i=1}^m \sum_{j=1}^m \scp{\ma_i}{\ma_j} x_i x_j\ =\ \mx^T \mA\mx. 
\]
T‰m‰n perusteella p‰‰tell‰‰n seuraavasti:
\[ 
\mA\mx = \mv{0} \qimpl \mx^T \mA\mx = 0 \qimpl \abs{\mw}^2 = 0 \qimpl \mw = \mv{0}. 
\]
Koska $\{\ma_1, \ldots, \ma_m\}$ oli lineaarisesti riippumaton systeemi, niin p‰‰tell‰‰n 
edelleen: $\ \mw=\mv{0}\,\ \impl\,\ \mx=\mv{0}$. \ On siis p‰‰telty:\,
$\mA\mx=\mv{0}\,\ \impl\,\ \mx=\mv{0}$, joten $\mA$ on s‰‰nnˆllinen. \loppu

Projektio-ongelman normaalimuodon ratkaisu $\mx = \mA^{-1}\mb$ antaa siis ortogonaaliprojektion
$\mw$ koordinaatit $W$:n kannassa $\{\ma_1, \ldots, \ma_m\}$. Ratkaisua varten on ensin 
laskettava matriisi \mA\ ja vektori \mb. N‰m‰ riippuvat vektoreista $\mpu$ ja $\{\ma_i\}$ sek‰
valitusta $\R^n$:n skalaaritulosta, joka  siis voi olla muukin kuin euklidinen. Vektorin $\mb$
riippuvuus vektorista $\mpu$ voidaan purkaa matriisialgebraksi: 
\[ 
\mpu = \sum_{j=1}^n u_j \me_j \qimpl [\mb]_i\ =\ \scp{\mv{u}}{\ma_i}\ 
               =\ \scp{\ma_i}{\mv{u}}\ =\ \sum_{j=1}^n \scp{\ma_i}{\me_j} u_j. 
\]
Siis $\mb = \mB\mpu$, miss‰ 
\[ 
[\mB]_{ij}\ =\ \scp{\ma_i}{\me_j}, \quad i = 1 \ldots m,\ j = 1 \ldots n. 
\] 
(Euklidisen skalaaritulon tapauksessa on $[\mB]_{ij} = [\ma_i]_j$). T‰m‰n tulkinnan mukaisesti
ortogonaaliprojektio $\mpu\map\mx$ on lineaarikuvaus tyyppi‰ $P:\ \R^n \kohti \R^m$ ja $P$:n
matriisi on $\mA^{-1}\mB$.
\begin{Exa} \label{projektioesimerkki} Laske vektorin $\mpu=[0,1,0,0]^T$ ortogonaaliprojektio
euklidisen skalaaritulon suhteen aliavaruuteen $W\subset\R^4$, jonka viritt‰v‰t vektorit
$\ma_1 = [1,1,1,1]^T$, $\ma_2 = [1,1,1,0]^T$ ja $\ma_3 = [0,1,1,1]^T$. M‰‰rit‰ myˆs 
yksikkˆvektori $\mv{n}\in\R^4$, joka on ortogonaalinen aliavaruutta $W$ vastaan. 
\end{Exa}
\ratk  Projektioperiaatteen mukaisesti vektorin $\mw = x_1 \ma_1 + x_2 \ma_2 + x_3 \ma_3$ 
koordinaatit $x_i$ m‰‰r‰ytyv‰t ratkaisemalla yht‰lˆryhm‰ $\mA\mx = \mb$, miss‰
\[ 
\mA=(\,\scp{\ma_i}{\ma_j}\,)=\begin{rmatrix} 4&3&3\\3&3&2\\3&2&3 \end{rmatrix}, \quad
\mb=\mB\mv{u}=\begin{rmatrix} 1&1&1&1\\1&1&1&0\\0&1&1&1 \end{rmatrix} 
              \begin{rmatrix} 0\\1\\0\\0 \end{rmatrix}\ =\ 
              \begin{rmatrix} 1\\1\\1 \end{rmatrix}. 
\]
Ratkaisu on $\,\mx = \frac{1}{2}\,[-1,1,1]^T$, joten kysytty projektio on
\[
\mw\ = \frac{1}{2}(-\ma_1 + \ma_2 + \ma_3)^T\ =\ \frac{1}{2}[0,1,1,0]^T.
\]
Vektori 
\[ 
\mv{n}\ =\ \lambda\,(\mpu-\mw)\ =\ \frac{1}{2}\lambda\,[0,1,-1,0]^T
\]
on ortogonaalinen $W$:t‰ vastaan, joten kysyttyj‰ yksikkˆvektoreita ovat
\[ 
\mv{n} = \pm \frac{1}{\sqrt{2}}\,[0,1,-1,0]^T. \qquad \loppu 
\]

\Harj
\begin{enumerate}

\item
a) Lineaarikuvaukselle $A:\,\R^3\map\R^3$ p‰tee: $A\ma=(0,2,1)$, $A\mb=(1,2,3)$ ja
$A\mc=(1,-1,0)$. Laske $A(3\ma-2\mb+\mc)$. \newline
b) Lineaarikuvaukselle $A:\,\R^2\map\R^2$ p‰tee: $A(1,1)=(3,-1)$ ja $A(2,-1)=(1,2)$. Laske
$A$:n matriisi. \newline
c) Funktiolle $\mf:\,\R^3\map\R^3$ p‰tee: $\mf(0,1,1)=(1,0,0)$, $\mf(1,0,1)=(1,1,0)$ ja
$\mf(1,-1,0)=(1,1,1)$. Voiko $\mf$ olla lineaarikuvaus?

\item
Olkoon $A:\,\R^n\map\R^n$ lineaarikuvaus ja $\ma_1,\ldots,\ma_m\in\R^n$. \newline
a) N‰yt‰, ett‰ jos $\{\ma_1,\ldots,\ma_m\}$ on lineaarisesti riippuva niin samoin on 
$\{A\ma_1,\ldots,A\ma_m\}$. \newline
b) N‰yt‰, ett‰ jos $\{A\ma_1,\ldots,A\ma_m\}$ on lineaarisesti riippumaton, siin samoin on
$\{\ma_1,\ldots,\ma_m\}$. \newline
 c) Oletetaan lis‰ksi, ett‰ $A$ on k‰‰ntyv‰. N‰yt‰, ett‰ t‰llˆin $\{\ma_1,\ldots,\ma_m\}$ on 
lineaarisesti riippuva/riippumaton t‰sm‰lleen kun $\{A\ma_1,\ldots,A\ma_m\}$ on lineaarisesti 
riippuva/riippumaton.

\item
Laske matriisimuotoiset koordinaattien muunnoskaavat $\mx=\mA\mx'$ ja $\mx'=\mB\mx$ seuraaville
$\R^n$:n tai vastaavan geometrisen vektoriavaruuden kannan vaihdoille. \vspace{1mm}\newline
a) \ $\{\me_1,\me_2\} \ext \{\me_1+9\me_2,6\me_1+8\me_2\}$ \vspace{1mm}\newline
b) \ $\{\vec i,\vec j\,\} \ext \{\vec i-2\vec j,3\vec i+\vec j\,\}$ \vspace{0.3mm}\newline
c) \ $\{\me_1,\me_2\} \ext \{\ma,\mb\},\ \ma=[5,-1]^T,\ \mb=[1,5]^T$ \vspace{1mm}\newline
d) \ $\{\ma,\mb\} \ext \{\mc,\md\},\,\ \ma=[1,1]^T,\ \mb=[1,2]^T,\ \mc=[2,1]^T,\ \md=[1,-1]^T$
\vspace{0.5mm}\newline
e) \ $\{\vec i,\vec j,\vec k\,\} \ext \{\vec i+2\vec j,3\vec j+4\vec k,6\vec i+5\vec k\,\}$
\vspace{1mm}\newline
f) \ $\,\{\me_1,\me_2,\me_3\} \ext \{\me_1+\me_2+\me_3,\me_1-\me_2+\me_3,-\me_1+\me_2+\me_3\}$
\vspace{0.8mm}\newline
g) \ $\{\me_1,\ldots,\me_4\} \ext \{\ma_1,\ldots,\,\ma_4\},\,\ \ma_i=\me_1+\ldots+\me_i,\ 
i=1 \ldots 4$ \vspace{1.5mm}\newline
h) \ $\{\me_1,\ldots,\me_n\}\ext\{\me_1+\me_2,\me_2+\me_3,\ldots,\me_{n-1}+\me_n,\me_1+\me_n\},\
     n=7$

\item
Tutki, ovatko seuraavat vektorit lineaarisesti riippumattomat.
\vspace{1mm}\newline
a) \ $[0,0,4,1],\ [2,1,-1,1],\ [1,-1,2,1],\ [1,2,1,1]$ \newline
b) \ $[1,-1,1,-1,1],\ [1,1,1,1,1],\ [1,1,-1,1,1],\ [1,1,1,-1,1]$ \newline
c) \ $[1,1,1,1,1,1,1,1,1],\ [0,1,1,1,1,1,1,1,1,1],\ \ldots,\ [0,0,0,0,0,0,0,0,1]$

\item \label{H-m-6: jatkuva kierto tasossa}
Konstruoi jatkuva, kannan ortonormeerauksen ja suunnistuksen s‰ilytt‰v‰ muunnos
$t\in[0,1]\map\{\ma(t),\mb(t)\}$ $\R^2$:n kannasta $\{\me_1,\me_2\}$ annettuun,
ortonormeerattuun ja positiivisesti suunnistettuun kantaan $\{\ma,\mb\}$. \newline
\kor{Vihje}: Esimerkki \ref{napakoordinaatisto ja matriisit}.

\item \label{H-m-6: kiertoja}
N‰yt‰, ett‰ seuraavissa $\R^3$:n koordinaattien muunnoskaavoissa on kyse koordinaatiston
kierrosta. Mik‰ on k‰‰nteinen muunnoskaava? Mink‰ pisteiden koordinaatit pysyv‰t kierrossa
ennallaan?
\begin{align*}
&\text{a)}\ \ \begin{bmatrix} x'\\y'\\z' \end{bmatrix} =
              \frac{1}{7} \begin{rmatrix} 3&-2&6\\2&-6&-3\\6&3&-2 \end{rmatrix}
              \begin{bmatrix} x\\y\\z \end{bmatrix} \\[2mm]
&\text{b)}\ \ \begin{bmatrix} x'\\y'\\z' \end{bmatrix} =
              \frac{1}{15} \begin{rmatrix} 10&-5&10\\-11&-2&10\\-2&-14&-5 \end{rmatrix}
              \begin{bmatrix} x\\y\\z \end{bmatrix}
\end{align*}

\item
Seuraavien matriisien sarakket ja rivit viritt‰v‰t er‰‰n $\R^n$:n aliavaruuden
($n=3,4,5$ tai $7$). M‰‰rit‰ tuetulla Gaussin algoritmilla ko.\ avaruuden dimensio ja jokin 
kanta.
\begin{align*}
&\text{a)}\ \ \begin{rmatrix} 1&5&0\\1&2&3\\1&3&2 \end{rmatrix} \qquad
 \text{b)}\ \ \begin{rmatrix} 3&4&-2&1\\2&1&3&-2\\1&-2&8&-5 \end{rmatrix} \\[3mm]
&\text{c)}\ \ \begin{rmatrix} 
              1&2&-3&1&5\\3&0&3&-3&3\\2&-1&4&-3&1\\0&2&-4&2&4 
              \end{rmatrix} \qquad
 \text{d)}\ \ \begin{rmatrix}
              1&0&-1&-1&3&3&5\\1&1&1&2&2&-1&3\\1&2&3&5&1&-5&1\\3&2&1&3&7&2&11
              \end{rmatrix}
\end{align*}

\item (*)
$\R^3$:n yleinen kierretty (ortonormeerattu ja positiivisesti suunnistettu) kanta
$\{\ma,\mb,\mc\}$ syntyy kiert‰m‰ll‰ kantaa $\{\me_1,\me_2,\me_3\}$ $x_1$-akselin ymp‰ri
kulma $\alpha$, kierron tulosta $x_2$-akselin ymp‰ri kulma $\beta$ ja lopuksi n‰iden kiertojen
tulosta $x_3$-akselin ymp‰ri kulma $\gamma$. M‰‰rit‰ vektorit $\ma,\mb,\mc$ kulmien
$\,\alpha,\beta,\gamma\,$ avulla. Mill‰ kulmien arvoilla syntyy a) pallokoordinaattikanta
$\{\me_r,\me_\theta,\me_\varphi\}$ vastaten pallonpintakoordinaatteja $\,\theta,\varphi\,$,\,
b) teht‰v‰n \ref{H-m-6: kiertoja}a kierretty kanta?

\item (*)
Olkoon $W\subset\R^4$ aliavaruus, jonka viritt‰v‰t vektorit $\ma=[1,1,1,1]$ ja 
$\mb=[1,1,-1,-1]$. M‰‰rit‰ vektorin $\mpu=[1,2,-1,3]$ ortogonaaliprojektio aliavaruuteen $W$ \
a) euklidisen skalaaritulon, \ b) skalaaritulon $\scp{\mx}{\my}=x_1y_1+x_2y_2+2x_3y_3+3x_4y_4\,$
mieless‰. Millaisen minimointiongelman muotoa
\[
\mx \in W:\ f(\mx)\ =\ \text{min!}
\]
n‰m‰ ortogonaaliprojektiot ratkaisevat?

\end{enumerate}
