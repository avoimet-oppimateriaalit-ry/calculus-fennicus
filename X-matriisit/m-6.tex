\section{Lineaarikuvaukset} \label{lineaarikuvaukset}
\alku
\index{lineaarikuvaus|vahv}
\index{funktio A!g@lineaarikuvaus|vahv}

Avaruuksien $\R^n$ ja $\R^m$ väliseksi \kor{lineaarikuvaukseksi} sanotaan funktiota tyyppiä
\[ 
\mv{f}:\ \R^n \kohti \R^m, \quad m,n \in \N, 
\]
($\DF_{\mv{f}}=\R^n,\ \RF_{\mv{f}}\subset\R^m$), jolle on voimassa ehto
\begin{equation} \label{lineaarikuvauksen ehto} 
\mv{f}(\alpha \mx + \beta \my)\ =\ \alpha \mv{f}(\mx) + \beta \mv{f}(\my), \quad 
                                         \mx,\my \in \R^n,\ \alpha,\beta \in \R.
\end{equation}
Lineaarikuvauksia tarkasteltaessa on tapana poiketa hieman tavallisista funktiomerkinnöistä: 
Lineaarikuvausta merkitään
\[ 
\mv{f}(\mx) = A \mx \quad \text{(lineaarikuvaus)}. 
\]
Oikealla siis jätetään sulkeet pois muuttujan ympäriltä, ja tämä ilmenee myös lukutavassa: 
Luetaan '$A\,x$' mieluummin kuin '$A$ $x$:ssä'. Lukutavan mukaisesti ajatellaan kuvaamisen 
(eli funktion arvon määräämisen) tapahtuvan siten, että $A$ \kor{operoi} $\mx$:ään. 
Operoinnin voi ymmärtää abstraktina kertolaskuna (= matriisin ja vektorin tulo), joka on
vektorien yhteenlaskun että skalaarilla kertomisen suhteen \kor{lineaarinen} eli toteutaa
tavanomaiset osittelulait näiden laskuoperaatioiden suhteen:
\index{lineaarisuus!c@lineaarikuvauksen}
\[ 
\boxed{ \quad\kehys A(\alpha \mx + \beta \my) 
            = \alpha A \mx + \beta A \my \quad \text{(lineaarikuvaus)}. \quad } 
\]
Lineaarikuvausten symboleina käytetään yleensä isoja kirjaimia $A,B$ jne. 

Jos \mA\ on matriisi kokoa $m \times n$, niin matriisialgebran sääntöjen perusteella on 
ilmeistä, että funktio $\mv{f}(\mx) = \mA \mx$ on lineaarikuvaus tyyppiä 
$\mv{f}:\ \R^n \kohti \R^m$. Osoittautuu, ettei muunlaisia, tätä tyyppiä olevia
lineaarikuvauksia olekaan:
\begin{Prop} \label{lineaarikuvaukset ja matriisit} Jos $\mv{f}:\ \R^n \kohti \R^m$ on 
lineaarikuvaus, niin on olemassa yksikäsitteinen matriisi \mA\ kokoa $m \times n$, siten että
$\mv{f}(\mx) = \mA\mx,\ \mx \in \R^n$. \end{Prop}
\tod Kun $\mx \in \R^n$ esitetään kannan $\{\me_1, \ldots, \me_n\}$ avulla muodossa
\[ 
\mx\ =\ \sum_{i=1}^n x_i \me_i, 
\]
niin $\mv{f}$:n lineaarisuuden ja matriisialgebran sääntöjen (Luku \ref{matriisialgebra}) 
nojalla
\[ 
\mv{f}(\mx)\ =\ \sum_{i=1}^n x_i \mv{f}(\me_i)\ =\ \mA\mx, \qquad 
                             \mA=[\mv{f}(\me_1) \ldots \mv{f}(\me_n)]. \loppu
\]

Proposition \ref{lineaarikuvaukset ja matriisit} perusteella lineaarikuvausten ja matriisien 
välillä on kääntäen yksikäsitteinen vastaavuus:
\[ 
\text{lineaarikuvaus}\ A:\ \R^n \kohti \R^m \ \ \longleftrightarrow\ \ 
            \text{matriisi}\ \mA\ \text{kokoa}\ m \times n\ \ (A\mx = \mA\mx).
\]
Jos matriisi \mA\ vastaa lineaarikuvausta $A$, niin vastaavuuteen viitataan sanomalla, että 
\index{lineaarikuvaus!lineaarikuvauksen matriisi}%
$\mA$ on ko.\ \kor{lineaarikuvauksen matriisi} (tai esitysmatriisi). Esitysmatriisin kautta 
voidaan siis lineaarikuvauksilla laskeminen aina palauttaa matriisialgebraan. Toisaalta voidaan
matriisin ja vektorin kertolasku aina haluttaessa nähdä lineaarisena kuvauksena, ts.\ erään 
lineaarikuvauksen laskusäännön soveltamisena. Erityisesti kahden matriisin tulo vastaa kahden
lineaarikuvauksen yhdistelyä:
\[ 
AB\ \vast\ \mA\mB. 
\]
Yhdistelyä koskevat rajoitukset ovat samat kuin matriisitulossa: On oletettava, että 
$B:\ \R^n \kohti \R^p$ ja  $A:\ \R^p \kohti \R^m$ joillakin $m,n,p\in\N$, jolloin myös tulo 
$\mA\mB$ on määritelty. 

\index{kzyzy@käänteiskuvaus}%
Lineaarikuvauksen $A:\ \R^n \kohti \R^m$ \kor{käänteiskuvauksen} $A^{-1}$ muodollinen
määritelmä on
\[ 
A\mx = \my \quad \ekv \quad \mx = A^{-1}\my. 
\]
Käänteiskuvaus on tällä tavoin määriteltävissä, mikäli $A$ on 1-1 ('yksi yhteen'), eli toteuttaa
ehdon
\[ 
A\mx_1 = A\mx_2 \quad \impl \quad \mx_1 = \mx_2. 
\]
Koska $A$ on lineaarinen, tämä on sama kuin ehto
\[ 
A\mx = \mv{0} \quad \impl \quad \mx = \mv{0}. 
\] 
Aiemmista tuloksista (Luku \ref{tuettu Gauss}) voidaan päätellä, että tämä ehto voi toteutua 
vain jos $m \ge n$ (välttämätön ehto). Jos ehto toteutuu, on $A$ \kor{kääntyvä}, ja 
käänteiskuvaus on tällöin määritelty kuvauksena tyyppiä $A^{-1}:\ \R^m \kohti \R^n$. 
Määrittelyjoukko ei kuitenkaan ole välttämättä koko $\R^m$, sillä käänteiskuvauksen määritelmän
mukaan $A^{-1}\my$ on määritelty vain sellaisille vektoreille $\my \in \R^m$, joille 
$\my = \mA\mx$ jollakin $\mx \in \R^n$, ts.\ yhtälöryhmällä $\mA\mx = \my$ on oltava ratkaisu. 
Aiempien tulosten perustella yhtälöryhmän $\mA\mx = \my$ ratkeavuus jokaisella $\my \in \R^m$ on
mahdollinen vain jos $m \le n$ (välttämätön ehto). Päätellään siis, että käänteiskuvaus $A^{-1}$
on on olemassa ja määritelty koko $\R^m$:ssä täsmälleen kun $m=n$ ja $A$:n matriisi $\mA$ 
(neliömatriisi) on säännöllinen. Käänteiskuvaus on tällöin lineaarikuvaus tyyppiä 
$A^{-1}:\,\R^n \kohti \R^n$ (samaa tyyppiä kuin $A$), ja käänteiskuvauksen matriisi on 
luonnollisesti $\mA^{-1}$.

Lineaarikuvauksen $A:\ \R^n \kohti \R^n$ käänteiskuvauksen lineaarisuutta käytetään usein
hyödyksi lineaarisia yhtälöryhmiä ratkaistaessa. Tyypillinen sovellutustilanne on sellainen,
että yhtälöryhmä $\mA\mx = \mb$ halutaan ratkaista useilla eri $\mb$:n arvoilla \mA:n pysyessä 
samana. Jos ratkaisua merkitään kuvauksena $\mb \map \mx$ (kuvauksen $\mx\map\mb=\mA\mx$ 
käänteiskuvaus) ja oletetaan, että tunnetaan ratkaisut $\mb_1 \map \mx_1$ ja 
$\mb_2 \map \mx_2$, niin lineaarisuuden perusteella tiedetään 
(vrt.\ Luku \ref{Gaussin algoritmi}, Esimerkki \ref{ristikkoesimerkki})
\[ 
\alpha \mb_1 + \beta \mb_2\ \longmapsto\ \alpha \mx_1 + \beta \mx_2 \quad (\alpha,\beta \in \R). 
\]

\subsection*{Kannan vaihto $\R^n$:ssä}
\index{kanta!b@kannan vaihto|vahv}

$\R^n$:n vektorisysteemiä (järjestettyä joukkoa) $\{\ma_1, \ldots \ma_m\}$ sanotaan 
\index{lineaarinen riippumattomuus}%
\kor{lineaarisesti riippumattomaksi}, jos pätee
\[ 
x_1 \ma_1 + \ldots + x_m \ma_m\ =\ \mv{0} \quad \impl \quad x_i = 0, \ \ i = 1 \ldots m. 
\]
Jos vektorit $\ma_i$ kootaan matriisin $\mA$ sarakkeiksi ($\mA$ kokoa $n \times m$), niin 
voidaan kirjoittaa $x_1 \ma_1 + \ldots + x_m \ma_m\ = \mA\mx$ 
(vrt.\ Luku \ref{matriisialgebra}), joten lineaarisen riippumattomuuden ehto saa muodon
\[ 
\mx \in \R^m\ \ja\ \mA\mx = \mv{0} \quad \impl \quad \mx = \mv{0}, \quad\quad \mA 
                          = [\ma_1, \ldots, \ma_m]. 
\]
Luvun \ref{tuettu Gauss} tulosten mukaan tämä ehto voi olla voimassa vain kun $m \le n$. Siis
$\R^n$:ssä on jokainen $n+1$ vektorin (tai useamman vektorin) muodostama systeemi 
\index{lineaarinen riippuvuus}%
\kor{lineaarisesti riippuva}. Toisaalta jos $m=n$, niin lineaarisesti riippumattoman systeemin
muodostaa ainakin $\R^n$:n luonnollinen kanta $\{\me_1, \ldots \me_n\}$, jota ym.\ ehdossa 
vastaa matriisi $\mA=\mI$. Luku $n$ on siis lineaarisen riippumattomuuden kannalta kriittinen
luku, ja tämä onkin jo aiemmin nimetty avaruuden $\R^n$
\index{dimensio}%
\kor{dimensioksi} (=ulotteisuus). Jos
$m=n$, niin ym.\ ehto toteutuu täsmälleen kun $\mA$ on säännöllinen matriisi 
(Lause \ref{säännöllisyyskriteerit}). Näin ollen, ja koska $\mA$ on säännöllinen täsmälleen kun
$\mA^T$ on säännöllinen (ks.\ Luku \ref{inverssi}), saadaan neliömatriisin säännöllisyydelle
seuraavat uudet (geometriset) tulkinnat:
\[ \boxed{ \begin{aligned} 
\ykehys\quad \mA\ \text{säännöllinen} \quad 
             &\ekv \quad \text{\mA:n sarakkeet lineaarisesti riippumattomat} \quad \\
\akehys      &\ekv \quad \text{\mA:n rivit lineaarisesti riippumattomat}. \quad
           \end{aligned} } \] 
Jos $\{\ma_1, \ldots, \ma_n\}$ on $\R^n$:n lineaarisesti riippumaton vektorisysteemi, eli 
matriisi $\mA = [\ma_1, \ldots, \ma_n]$ on säännöllinen, niin mielivaltainen vektori 
$\mb \in \R^n$ voidaan lausua yksikäsitteisesti vektorien $\ma_i$ lineaarisena yhdistelynä. 
Nimittäin kertoimet $x_i$ ratkeavat yksikäsitteisesti yhtälöryhmästä
\[ 
\sum_{i=1}^n x_i \ma_i = \mb \quad \ekv \quad \mA\mx = \mb.
\]
Tällä perusteella voidaan sanoa, että jokainen lineaarisesti riippumaton $n$ vektorin systeemi
$\{\ma_1, \ldots, \ma_n\}$ on $\R^n$:n
\index{kanta}%
\kor{kanta}. Jos siis \mA\ on mikä tahansa säännöllinen
matriisi kokoa $n \times n$, niin $\mA$:n sarakkeet (myös rivit) muodostavat $\R^n$:n kannan. 
--- Huomattakoon, että tällainen kanta on
\index{kanta!a@ortonormeerattu}%
\kor{ortonormeerattu}, eli 
$\scp{\ma_i}{\ma_j} = \delta_{ij}$ (Kroneckerin $\delta$), täsmälleen kun \mA\ on ortogonaalinen
(vrt.\ Luku \ref{inverssi}). Päätellään siis, että jokainen säännöllinen matriisi kokoa 
$n \times n$ edustaa sarakkeittensa (tai riviensä) kautta $\R^n$:n kantaa, ja jokainen 
ortogonaalinen matriisi edustaa ortonormeerattua kantaa.

Jos halutaan suorittaa \kor{kannan vaihto}, lähtökohtana $\R^n$:n luonnollinen kanta 
$\{\me_1, \ldots, \me_n\}$, niin vektorin $\mx$ koordinaatit $x'_i$ kannassa 
$\{\ma_1, \ldots \ma_m\}$ ovat siis ratkaistavissa yhtälöryhmästä
\[ 
\mA\mx' = \mx \quad \impl \quad \mx' = \mA^{-1}\mx, \quad\quad \mA = [\ma_1, \ldots, \ma_n]. 
\]
Kannan vaihdossa, eli koordinaattimuunnoksessa $\mx \map \mx'$, on siis kyse 
lineaarikuvauksesta. Jos uusikin kanta on ortonormeerattu, niin muunnoksen matriisi on helppo
laskea: $\mA^{-1}=\mA^T$.
\begin{Exa} Määritä vektorin $x \vec{i} + y \vec{j} + z \vec{k}$ koordinaatit $x',y',z'$ 
kannassa, jonka muodostavat vektorit $\vec{a}_1 = \vec{i} + \vec{j}$, 
$\ \vec{a}_2 = \vec{j} + \vec{k}$ ja $\vec{a}_3 = \vec{i} + \vec{k}$.
\end{Exa}
\ratk 
\[ 
\begin{rmatrix} 1&0&1\\1&1&0\\0&1&1 \end {rmatrix} \begin{rmatrix} x'\\y'\\z' \end{rmatrix} =
\begin{rmatrix} x\\y\\x \end{rmatrix} \quad \impl \quad 
\begin{rmatrix} x'\\y'\\z' \end{rmatrix} =
     \frac{1}{2} \begin{rmatrix} 1&1&-1\\-1&1&1\\1&-1&1 \end{rmatrix} 
                 \begin{rmatrix} x\\y\\z \end{rmatrix}. \loppu
\]
\begin{Exa} \label{pallokoordinaatisto ja matriisit} 
Pallokoordinaatiston pisteessä $P=(r,\theta,\varphi)$ käytetään avaruusvektoreille
kantaa $\{\vec e_r,\vec e_\theta,\vec e_\varphi\}$, missä (vrt.\ Luku \ref{koordinaatistot})
\[
\left\{\begin{array}{ll}
\vec e_r       &= \ \sin{\theta} \cos{\varphi} \vec i + \sin{\theta}\sin{\varphi} \vec j 
                                                      + \cos{\theta} \vec k, \quad \\[4pt]
\vec e_\theta  &= \ \cos{\theta} \cos{\varphi} \vec i + \cos{\theta}\sin{\varphi} \vec j 
                                                      - \sin{\theta} \vec k, \\[4pt]
\vec e_\varphi &= \ -\sin{\varphi} \vec i + \cos{\varphi} \vec j.
\end{array} \right.
\]
Kun kirjoitetaan
\[
\vec F \,=\, F_1\vec i+F_2\vec j+F_3\vec k
       \,=\, F_r\vec e_r+F_\theta\vec e_\theta+F_\varphi\vec e_\varphi\,,
\]
niin muunnoskaava $(F_r,F_\theta,F_\varphi) \map (F_1,F_2,F_3)$ on matriisimuodossa
\[
\begin{rmatrix}
\sin\theta\cos\varphi&\cos\theta\cos\varphi&-\sin\varphi\\
\sin\theta\sin\varphi&\cos\theta\sin\varphi&\cos\varphi \\
\cos\theta&-\sin\theta&0
\end{rmatrix}
\begin{bmatrix} F_r\\F_\theta\\F_\varphi \end{bmatrix} =
\begin{bmatrix} F_1\\F_2\\F_3 \end{bmatrix}.
\]
Koska $\{\vec e_r,\vec e_\theta,\vec e_\varphi\}$ on ortonormeerattu systeemi, niin
käänteismuunnos saadaan transponoimalla:
\[
\begin{bmatrix} F_r\\F_\theta\\F_\varphi \end{bmatrix} =
\begin{rmatrix}
\sin\theta\cos\varphi&\sin\theta\sin\varphi&\cos\theta\\
\cos\theta\cos\varphi&\cos\theta\sin\varphi&-\sin\theta \\
-\sin\varphi&\cos\varphi&0
\end{rmatrix}
\begin{bmatrix} F_1\\F_2\\F_3 \end{bmatrix}. \loppu
\]
\end{Exa}

\subsection*{Koordinaatiston kierto}
\index{koordinaatisto!d@koordinaatiston kierto|vahv}
\index{kierto!b@koordinaatiston|vahv}

Edellä todettiin, että jos $\mA$ on ortogonaalinen matriisi kokoa $n \times n$, niin $\mA$:n
sarakkeet (myös rivit) määrittelevät $\R^n$:n ortonormeeratun kannan. Sanotaan, että ko.\
\index{suunnistus (kannan)} \index{positiivinen suunnistus!a@kannan} 
\index{negatiivisesti!a@suunnistettu (kanta)}%
kanta on \kor{positiivisesti suunnistettu} (tai 'oikeakätinen'), jos $\det\mA=1$ ja
\kor{negatiivisesti suunnistettu} ('vasenkätinen'), jos $\det\mA=-1$. (Muita mahdollisia
arvoja ei $\det\mA$:lla ole, ks. Harj.teht. 
\ref{determinantti}:\,\ref{H-m-5: ortogonaalisen matriisin determinantti}). Jos yhden 
kantavektorin suunta vaihdetaan, niin determinanttiopin mukaisesti $\det\mA$:n merkki vaihtuu,
eli kannan suunnistus vaihtuu.
\begin{Exa} Jos $\{\vec a,\vec b,\vec c\,\}$ on avaruusvektoreiden ortonormeerattu kanta, niin
ko.\ vektoreiden koordinaattivektorit kannassa $\{\vec i,\vec j,\vec k\,\}$ muodostavat
$\R^3$:n ortonormeeratun kannan $\{\ma,\mb,\mc\}$, eli matriisi $\mA=[\ma\,\mb\,\mc]$ on
ortogonaalinen (kuten edellisessä esimerkissä). Geometrisin vektorioperaatioin laskettuna
$\mA$:n determinantti on (vrt.\ Luku \ref{ristitulo})
\[
\det\mA=\det\mA^T=(\vec a\times\vec b)\cdot\vec c=[\,\vec a\,\vec b\,\vec c\,].
\]
Koska $\{\vec a,\vec b,\vec c\,\}$ on ortonormeerattu systeemi, niin on joko 
$\vec c=\vec a\times\vec b$, jolloin $\det\mA=\abs{\vec c\,}^2=1$, tai 
$\vec c=-(\vec a\times\vec b)$, jolloin $\det\mA=-1$. Edellisessä tapauksessa kanta on siis 
positiivisesti, jälkimmäisessä negatiivisesti suunnistettu. \loppu
\end{Exa}
\begin{Exa} \label{napakoordinaatisto ja matriisit} Napakoordinaatiston kannat 
$\{\vec e_r,\vec e_\varphi\}$ (vrt.\ Luku \ref{koordinaatistot}) kattavat eri $\varphi$:n 
arvoilla kaikki mahdolliset tason vektoreiden ortonormeeratut, positiivisesti suunnistetut
systeemit. Näin ollen kaikki ortogonaaliset matriisit $\mA$ kokoa $2 \times 2$, jotka täyttävät
ehdon $\det\mA=1$, ovat muotoa
\[
\mA = \begin{rmatrix} 
      \cos\varphi&\sin\varphi\\-\sin\varphi&\cos\varphi
      \end{rmatrix}, \quad \varphi\in[0,2\pi). \loppu
\]
\end{Exa}

Jos $\mA=[\ma_1 \ldots \ma_n]$ on ortogonaalinen matriisi ja $\det\mA=1$, niin kannan vaihtoa
$\{\me_1,\ldots,\me_n\} \ext \{\ma_1,\ldots,\ma_n\}$ sanotaan \kor{koordinaatiston kierroksi}.
Nimittäin on osoitettavissa, että kannan vaihto on tällöin toteutettavissa j\pain{atkuvana} 
muunnoksena muotoa
\[
t\in[0,1]\ \map\ \{\ma_1(t),\ldots\,\ma_n(t)\}
\]
siten, että seuraavat ehdot toteutuvat:
\begin{itemize}
\item[(i)]   $\ma_i(0)=\me_i$ ja $\ma_i(1)=\ma_i$, $\ i=1 \ldots n$.
\item[(ii)]  Vektorisysteemi $\{\ma_1(t),\ldots,\ma_n(t)\}$ on ortonormeerattu ja positiivisesti
             suunnistettu jokaisella $t\in[0,1]$.
\item[(iii)] Jatkuvuusehto: Funktiot $t\in[0,1]\map\ma_j(t)\in\R^n$ ovat jatkuvia, ts.\
             reaalifunktiot $t \map a_{ij}(t)=[\ma_j(t)]_i,\ i,j=1 \ldots n\,$ ovat jatkuvia 
             välillä $[0,1]$. 
\end{itemize}
Ehdot täyttävä muunnos voidaan kuvitella toteutettavan niin, että vektorisysteemiä
$\{\me_1,\ldots,\me_n\}$ kierretään 'jäykkänä kappaleena', jolloin ortogonaalisuus ja suunnistus
säilyvät. Perussysteemistä $\{\me_1,\ldots,\me_n\}$ lähtien on siis tällaisella jatkuvalla 
kierrolla mahdollista päätyä mihin tahansa annettuun, ortonormeerattuun ja positiivisesti 
suunnistettuun systeemiin $\{\ma_1,\ldots,\ma_n\}$. Jos ko.\ systeemi on negatiivisesti 
suunnistettu, niin jossakin on tapahduttava 'hyppy' (yhden vektorin suunnan vaihto), jolloin 
muunnos ei ole jatkuva. Tämän väittämän mukaisesti siis kaikki $\R^n$:n positiivisesti 
suunnistetut ortonormeeratut systeemit ovat keskenään (samoin negatiivisesti suunnistetut 
\index{kiertoekvivalenssi}%
keskenään) \kor{kiertoekvivalentteja}. Tapauksessa $n=2$ väittämän voi todistaa
Esimerkin \ref{napakoordinaatisto ja matriisit} tuloksen perusteella 
(Harj.teht.\,\ref{H-m-6: jatkuva kierto tasossa}). Yleinen tapaus on haastavampi --- tarkemmat
perustelut sivuutetaan.

\subsection*{$\R^n$:n aliavaruudet}
\index{aliavaruus|vahv}

Olkoon annettu $m$ vektoria $\ma_j\in\R^n,\ j=1 \ldots m$ (voi olla $m \le n$ tai $m>n$).
Tällöin vektorijoukko, joka määritellään
\[
W = \{\mpv\in\R^n \mid \mpv=\sum_{j=1}^m y_j\ma_j\,\ 
                                   \text{jollakin}\,\ [y_1,\ldots,y_m]^T\in\R^m\}
\]
on $\R^n$:n \kor{aliavaruus}, sillä määritelyn perusteella pätee $W\subset\R^n$ ja
\[
\mpv_1,\mpv_2 \in W \qimpl \alpha_1\mpv_1+\alpha_2\mpv_2 \in W \quad 
                                          \forall\ \alpha_1,\alpha_2\in\R.
\]
\index{virittää (aliavaruus)}%
Sanotaan, että $W$ on vektoreiden $\ma_1,\ldots,\ma_m$ \kor{virittämä} avaruus. Jos vektorit
$\ma_i$ ovat lineaarisesti riippumattomat (jolloin on oltava $m \le n$), niin dim W $=m$ ja 
$\{\ma_1,\ldots\ma_m\}$ on $W$:n kanta. Ovatko vektorit $\ma_i$ lineaarisesti riippumattomat vai
eivät, selviää Gaussin algoritmilla: Muodostetaan vektorit $\ma_j$ sarakkeina matriisi 
$\mA=[\ma_1\,\ldots\,\ma_m]$ ja etsitään yhtälöryhmän $\mA\mx=\mo$ kaikki ratkaisut $\mx\in\R^m$
(vrt.\ Luku \ref{tuettu Gauss}).
\begin{Exa} Ovatko $\R^4$:n vektorit $\ma_1 = [1,2,1,-1]^T$, $\ma_2 = [2,-1,3,-1]^T$ ja 
$\ma_3 = [0,5,-1,-1]^T$ lineaarisesti riippumattomat? 
\end{Exa}
\ratk Muunnetaan yhtälöryhmä $\mA\mx = \mv{0}$ Gaussin algoritmilla singulaariseen perusmuotoon
$\mU\mx = \mv{0}$:
\[ 
\begin{rmatrix} 1&2&0\\2&-1&5\\1&3&-1\\-1&-1&-1 \end{rmatrix} 
\begin{rmatrix} x_1\\x_2\\x_3 \end{rmatrix}\ =\ 
\begin{rmatrix} 0\\0\\0\\0 \end{rmatrix} \quad \ekv \quad
\begin{rmatrix} 1&2&0\\0&-5&-5\\0&0&0\\0&0&0 \end{rmatrix} 
\begin{rmatrix} x_1\\x_2\\x_3 \end{rmatrix}\ =\ \begin{rmatrix} 0\\0\\0\\0 \end{rmatrix}. 
\]
Tämän perusteella
\[ 
\mA\mx = \mv{0} \quad \ekv \quad \mx = t\,[-2,1,1]^T,\ t \in \R. 
\] 
Vastaus: Eivät, sillä $-2\ma_1+\ma_2+\ma_3 = \mv{0}$. \loppu 

Gaussin algoritmilla selviää myös yleisemmin, mikä on annettujen vektorien
$\ma_j\in\R^n,\ j=1 \ldots m$ virittämän aliavaruuden $W$ dimensio ja mikä ko.\
vektorisysteemin lineaarisesti riippumaton \pain{osas}y\pain{steemi} kelpaa $W$:n kannaksi 
silloin, kun koko systeemi on lineaarisesti riippuva.
\jatko \begin{Exa} (jatko) Esimerkin matriisin $\mA$ sarakkeet virittävät $\R^4$:n aliavaruuden
$W$ ja rivit $\R^3$:n aliavaruuden $U$. Määritä näiden aliavaruuksien dimensiot ja kummallekin 
jokin kanta.
\end{Exa}
\ratk
Esimerkin algoritmissa ei tehty rivien eikä sarakkeiden vaihtoja, joten tuloksesta on suoraan 
luettavissa: dim $W=$ dim $U=2$, $W$:n eräs kanta $=\mA$:n sarakkeet $1$ ja $2$ ja $U$:n eräs 
kanta $=\mA$:n rivit $1$ ja $2$, ts.\
\begin{align*}
W &= \{\mpv=\alpha_1[1,2,1,-1]^T+\alpha_2[2,-1,3,-1]^T,\,\ \alpha_1,\alpha_2\in\R\}, \\
U &= \{\mpv=\alpha_1[1,2,0]+\alpha_2[2,-1,5],\,\ \alpha_1,\alpha_2\in\R\}. \loppu
\end{align*}
\begin{Exa} Matriisin
\[
\mA=\begin{rmatrix}
    3&3&0&3&4&1\\1&-3&2&-1&-1&2\\-3&3&-3&0&2&-1\\6&-2&4&2&1&3
    \end{rmatrix}
\]
sarakkeet virittävät $\R^4$:n aliavaruuden $W$ ja rivit $\R^6$:n aliavaruuden $U$. Määritä
näiden aliavaruuksien dimensiot ja kummallekin jokin kanta.
\end{Exa}
\ratk Yhtälöryhmän
\[
\begin{rmatrix}
3&3&0&3&4&1\\1&-3&2&-1&-1&2\\-3&3&-3&0&2&-1\\6&-2&4&2&1&3
\end{rmatrix}
\begin{bmatrix} x_1\\x_2\\x_3\\x_4\\x_5\\x_6 \end{bmatrix} =
\begin{bmatrix} 0\\0\\0\\0\\0\\0 \end{bmatrix}
\]
singulaariseksi perusmuodoksi saadaan tuetulla Gaussin algoritmilla
\[
\begin{rmatrix}
3&3&4&1&0&3 \\[1mm] 0&-4&-\frac{7}{3}&\frac{5}{3}&2&-2 \\[1mm]
0&0&\frac{5}{2}&\frac{5}{2}&0&0 \\[1mm] 0&0&0&0&0&0
\end{rmatrix} 
\begin{bmatrix} x_1\\x_2\\x_5\\x_6\\x_3\\x_4 \end{bmatrix} =
\begin{bmatrix} 0\\0\\0\\0\\0\\0 \end{bmatrix}.
\]
Tähän muotoon pääsemiseksi ei ole tehty rivinvaihtoja. Sarakkeita sen sijaan on vaihdettu niin,
että uusi järjestys on $125634$, kuten näkyy lopputuloksesta. Päätellään siis: 
dim $W=$ dim $U=3$, $U$:n erään kannan muodostavat $\mA$:n kolme ensimmäistä riviä ja $W$:n 
erään kannan $\mA$:n sarakkeet $1$, $2$ ja $5$. \loppu  

Em.\ esimerkeissä matriisin sarakkeiden ja rivien virittämien avaruuksien dimensiot ovat samat.
Kyse ei ole sattumasta, vaan dimensiot ovat \pain{aina} samat: dimensioiden yhteinen arvo = 
matriisin säännöllisyysaste (ks.\ Luku \ref{tuettu Gauss}).
 
\subsection*{*Ortogonaaliprojektio $\R^n$:ssä}
\index{ortogonaaliprojektio|vahv}

Olkoon $\{\ma_1, \ldots, \ma_m\},\ 1 \le m < n$, lineaarisesti riippumaton vektorisysteemi 
$\R^n$:ssä, ja olkoon $W$ ko.\ vektorien virittämä $R^n$:n (aito) aliavaruus:
\[ 
W = \{\mv{v} \in \R^n \mid \mv{v} = \sum_{i=1}^m x_i \ma_i,\ \ x_i \in \R \}. 
\]
Tällöin jos $\mv{u} \in \R^n$, niin $\mv{u}$:n \kor{ortogonaaliprojektio} aliavaruuteen $W$ 
määritellään vektorina $\mw$, joka toteuttaa
\[ 
\mw \in W\ \ \ja\ \ \scp{\mpu-\mw}{\mpv} = 0\ \ \forall\ \mpv \in W. 
\]
Kirjoittamalla tässä $\mpv$ vektoreiden $\ma_1, \ldots, \ma_m$ lineaarikombinaationa 
(mahdollista jokaiselle $\mpv \in W$) ja käyttämällä skalaaritulon bilineaarisuutta nähdään,
että ortogonaalisuusehto riittää asettaa vektoreille $\mpv=\ma_i,\ i= 1\,\ldots\,m\,$:
\[ 
\scp{\mpu-\mw}{\mpv} = 0\ \ \forall\ \mpv \in W  
               \qekv \scp{\mpu-\mw}{\ma_i} = 0,\ \ i = 1 \ldots m. 
\]
Ortogonaaliprojektion haku on näin pelkistetty yhtälöryhmäksi. Kun tässä kirjoitetaan vielä
$\mw = \sum_{j=1}^m x_j \ma_j$ (mahdollista, koska $\mw \in W$), ja käytetään edelleen 
skalaaritulon bilineaarisuutta ja symmetrisyyttä, niin nähdään, että kyseessä on seuraava 
lineaarinen yhtälöryhmä kerroinvektorille $\mx = [x_1, \ldots, x_m]^T\in\R^m$\,:
\[ 
\boxed{\quad\kehys \mA\mx = \mb, \qquad 
              [\mA]_{ij} = \scp{\ma_i}{\ma_j}, \quad [\mb]_i = \scp{\mpu}{\ma_i}, 
                                               \quad i,j = 1 \ldots m. \quad } 
\]
\index{normaalimuoto!b@projektio-ongelman}%
Tätä muotoilua sanotaan projektio-ongelman \kor{normaalimuodoksi}. 

Projektio-ongelmaan päädytään lineaarialgebran sovelluksissa usein minimointiongelman kautta:
Halutaan etsiä annettua vektoria $\mpu\in\R^n$ jonkin \pain{normin} $\norm{\cdot}$ mielessä 
\pain{lähin} vektori aliavaruudesta $W$, ts.\ halutaan ratkaista minimointiongelma
\[ 
\quad \mpv \in W: \quad \norm{\mpu-\mpv} = \text{min!} 
\]
Jos normi liittyy skalaarituloon $\,\scp{\cdot}{\cdot}\,$ siten, että 
$\norm{\mx}^2=\scp{\mx}{\mx}$ (ks.\ Luku \ref{abstrakti skalaaritulo}), niin ongelman ratkaisu
saadaan $\mpu$:n ortogonaaliprojektiona $\mw \in W$ ko.\ skalaaritulon suhteen. Nimittäin jos
$\mpv \in W$ on mielivaltainen ja merkitään $\mpv_0 = \mw-\mpv$, niin
$\scp{\mpu-\mw}{\mpv_0}=0$ (koska $\mpv_0 \in W$), joten seuraa
\begin{align*}
\norm{\mpu-\mpv}^2 &=\ \norm{(\mpu-\mw)+\mpv_0)}^2\ 
                    =\ \scp{\,(\mpu-\mw)+\mpv_0\,}{\,(\mpu-\mw)+\mpv_0\,} \\
                   &=\ \abs{\mpu-\mw}^2 + 2\,\scp{\mpu-\mw}{\mpv_0} + \abs{\mpv_0}^2 \\
                   &=\ \abs{\mpu-\mw}^2 + \abs{\mpv_0}^2.
\end{align*}
Siis $\norm{\mpu-\mpv}$:n minimiarvo saavutetaan, kun $\mpv_0=\mv{0}\ \ekv\ \mpv = \mw$.

Projektio-ongelman yksikäsitteisen ratkeavuuden takaa
\begin{Lause} \label{projektiolause} (\vahv{$\R^n$:n projektiolause}) Jos $W \subset \R^n$ on
lineaarisesti riippumattomien vektorien $\ma_1, \ldots, \ma_m$ virittämä $\R^n$:n aliavaruus 
($m<n$) ja $\mpu \in \R^n$, niin on olemassa yksikäsitteinen $\mw \in W$ siten, että 
$\ \scp{\mpu-\mw}{\mpv} = 0\ \ \forall\ \mpv \in W$. 
\end{Lause}
\tod Lause väittää, että matriisi $\mA\ = (\scp{\ma_i}{\ma_j})$ 
(neliömatriisi kokoa $m \times m$) on säännöllinen. Tämän näyttämiseksi olkoon
$\mw=\sum_{i=1}^m x_i\ma_i$ ja lasketaan ensin
\[ 
\abs{\mw}^2\  =\ \scp{\mw}{\mw}\ =\ \scp{\,\sum_{i=1}^m x_i \ma_i}{\sum_{j=1}^m x_j \ma_j\,}. 
\]
Skalaaritulon bilineaarisuuden ja matriisialgebran nojalla tämä purkautuu muotoon
\[ 
\abs{\mw}^2\ =\ \sum_{i=1}^m \sum_{j=1}^m \scp{\ma_i}{\ma_j} x_i x_j\ =\ \mx^T \mA\mx. 
\]
Tämän perusteella päätellään seuraavasti:
\[ 
\mA\mx = \mv{0} \qimpl \mx^T \mA\mx = 0 \qimpl \abs{\mw}^2 = 0 \qimpl \mw = \mv{0}. 
\]
Koska $\{\ma_1, \ldots, \ma_m\}$ oli lineaarisesti riippumaton systeemi, niin päätellään 
edelleen: $\ \mw=\mv{0}\,\ \impl\,\ \mx=\mv{0}$. \ On siis päätelty:\,
$\mA\mx=\mv{0}\,\ \impl\,\ \mx=\mv{0}$, joten $\mA$ on säännöllinen. \loppu

Projektio-ongelman normaalimuodon ratkaisu $\mx = \mA^{-1}\mb$ antaa siis ortogonaaliprojektion
$\mw$ koordinaatit $W$:n kannassa $\{\ma_1, \ldots, \ma_m\}$. Ratkaisua varten on ensin 
laskettava matriisi \mA\ ja vektori \mb. Nämä riippuvat vektoreista $\mpu$ ja $\{\ma_i\}$ sekä
valitusta $\R^n$:n skalaaritulosta, joka  siis voi olla muukin kuin euklidinen. Vektorin $\mb$
riippuvuus vektorista $\mpu$ voidaan purkaa matriisialgebraksi: 
\[ 
\mpu = \sum_{j=1}^n u_j \me_j \qimpl [\mb]_i\ =\ \scp{\mv{u}}{\ma_i}\ 
               =\ \scp{\ma_i}{\mv{u}}\ =\ \sum_{j=1}^n \scp{\ma_i}{\me_j} u_j. 
\]
Siis $\mb = \mB\mpu$, missä 
\[ 
[\mB]_{ij}\ =\ \scp{\ma_i}{\me_j}, \quad i = 1 \ldots m,\ j = 1 \ldots n. 
\] 
(Euklidisen skalaaritulon tapauksessa on $[\mB]_{ij} = [\ma_i]_j$). Tämän tulkinnan mukaisesti
ortogonaaliprojektio $\mpu\map\mx$ on lineaarikuvaus tyyppiä $P:\ \R^n \kohti \R^m$ ja $P$:n
matriisi on $\mA^{-1}\mB$.
\begin{Exa} \label{projektioesimerkki} Laske vektorin $\mpu=[0,1,0,0]^T$ ortogonaaliprojektio
euklidisen skalaaritulon suhteen aliavaruuteen $W\subset\R^4$, jonka virittävät vektorit
$\ma_1 = [1,1,1,1]^T$, $\ma_2 = [1,1,1,0]^T$ ja $\ma_3 = [0,1,1,1]^T$. Määritä myös 
yksikkövektori $\mv{n}\in\R^4$, joka on ortogonaalinen aliavaruutta $W$ vastaan. 
\end{Exa}
\ratk  Projektioperiaatteen mukaisesti vektorin $\mw = x_1 \ma_1 + x_2 \ma_2 + x_3 \ma_3$ 
koordinaatit $x_i$ määräytyvät ratkaisemalla yhtälöryhmä $\mA\mx = \mb$, missä
\[ 
\mA=(\,\scp{\ma_i}{\ma_j}\,)=\begin{rmatrix} 4&3&3\\3&3&2\\3&2&3 \end{rmatrix}, \quad
\mb=\mB\mv{u}=\begin{rmatrix} 1&1&1&1\\1&1&1&0\\0&1&1&1 \end{rmatrix} 
              \begin{rmatrix} 0\\1\\0\\0 \end{rmatrix}\ =\ 
              \begin{rmatrix} 1\\1\\1 \end{rmatrix}. 
\]
Ratkaisu on $\,\mx = \frac{1}{2}\,[-1,1,1]^T$, joten kysytty projektio on
\[
\mw\ = \frac{1}{2}(-\ma_1 + \ma_2 + \ma_3)^T\ =\ \frac{1}{2}[0,1,1,0]^T.
\]
Vektori 
\[ 
\mv{n}\ =\ \lambda\,(\mpu-\mw)\ =\ \frac{1}{2}\lambda\,[0,1,-1,0]^T
\]
on ortogonaalinen $W$:tä vastaan, joten kysyttyjä yksikkövektoreita ovat
\[ 
\mv{n} = \pm \frac{1}{\sqrt{2}}\,[0,1,-1,0]^T. \qquad \loppu 
\]

\Harj
\begin{enumerate}

\item
a) Lineaarikuvaukselle $A:\,\R^3\map\R^3$ pätee: $A\ma=(0,2,1)$, $A\mb=(1,2,3)$ ja
$A\mc=(1,-1,0)$. Laske $A(3\ma-2\mb+\mc)$. \newline
b) Lineaarikuvaukselle $A:\,\R^2\map\R^2$ pätee: $A(1,1)=(3,-1)$ ja $A(2,-1)=(1,2)$. Laske
$A$:n matriisi. \newline
c) Funktiolle $\mf:\,\R^3\map\R^3$ pätee: $\mf(0,1,1)=(1,0,0)$, $\mf(1,0,1)=(1,1,0)$ ja
$\mf(1,-1,0)=(1,1,1)$. Voiko $\mf$ olla lineaarikuvaus?

\item
Olkoon $A:\,\R^n\map\R^n$ lineaarikuvaus ja $\ma_1,\ldots,\ma_m\in\R^n$. \newline
a) Näytä, että jos $\{\ma_1,\ldots,\ma_m\}$ on lineaarisesti riippuva niin samoin on 
$\{A\ma_1,\ldots,A\ma_m\}$. \newline
b) Näytä, että jos $\{A\ma_1,\ldots,A\ma_m\}$ on lineaarisesti riippumaton, siin samoin on
$\{\ma_1,\ldots,\ma_m\}$. \newline
 c) Oletetaan lisäksi, että $A$ on kääntyvä. Näytä, että tällöin $\{\ma_1,\ldots,\ma_m\}$ on 
lineaarisesti riippuva/riippumaton täsmälleen kun $\{A\ma_1,\ldots,A\ma_m\}$ on lineaarisesti 
riippuva/riippumaton.

\item
Laske matriisimuotoiset koordinaattien muunnoskaavat $\mx=\mA\mx'$ ja $\mx'=\mB\mx$ seuraaville
$\R^n$:n tai vastaavan geometrisen vektoriavaruuden kannan vaihdoille. \vspace{1mm}\newline
a) \ $\{\me_1,\me_2\} \ext \{\me_1+9\me_2,6\me_1+8\me_2\}$ \vspace{1mm}\newline
b) \ $\{\vec i,\vec j\,\} \ext \{\vec i-2\vec j,3\vec i+\vec j\,\}$ \vspace{0.3mm}\newline
c) \ $\{\me_1,\me_2\} \ext \{\ma,\mb\},\ \ma=[5,-1]^T,\ \mb=[1,5]^T$ \vspace{1mm}\newline
d) \ $\{\ma,\mb\} \ext \{\mc,\md\},\,\ \ma=[1,1]^T,\ \mb=[1,2]^T,\ \mc=[2,1]^T,\ \md=[1,-1]^T$
\vspace{0.5mm}\newline
e) \ $\{\vec i,\vec j,\vec k\,\} \ext \{\vec i+2\vec j,3\vec j+4\vec k,6\vec i+5\vec k\,\}$
\vspace{1mm}\newline
f) \ $\,\{\me_1,\me_2,\me_3\} \ext \{\me_1+\me_2+\me_3,\me_1-\me_2+\me_3,-\me_1+\me_2+\me_3\}$
\vspace{0.8mm}\newline
g) \ $\{\me_1,\ldots,\me_4\} \ext \{\ma_1,\ldots,\,\ma_4\},\,\ \ma_i=\me_1+\ldots+\me_i,\ 
i=1 \ldots 4$ \vspace{1.5mm}\newline
h) \ $\{\me_1,\ldots,\me_n\}\ext\{\me_1+\me_2,\me_2+\me_3,\ldots,\me_{n-1}+\me_n,\me_1+\me_n\},\
     n=7$

\item
Tutki, ovatko seuraavat vektorit lineaarisesti riippumattomat.
\vspace{1mm}\newline
a) \ $[0,0,4,1],\ [2,1,-1,1],\ [1,-1,2,1],\ [1,2,1,1]$ \newline
b) \ $[1,-1,1,-1,1],\ [1,1,1,1,1],\ [1,1,-1,1,1],\ [1,1,1,-1,1]$ \newline
c) \ $[1,1,1,1,1,1,1,1,1],\ [0,1,1,1,1,1,1,1,1,1],\ \ldots,\ [0,0,0,0,0,0,0,0,1]$

\item \label{H-m-6: jatkuva kierto tasossa}
Konstruoi jatkuva, kannan ortonormeerauksen ja suunnistuksen säilyttävä muunnos
$t\in[0,1]\map\{\ma(t),\mb(t)\}$ $\R^2$:n kannasta $\{\me_1,\me_2\}$ annettuun,
ortonormeerattuun ja positiivisesti suunnistettuun kantaan $\{\ma,\mb\}$. \newline
\kor{Vihje}: Esimerkki \ref{napakoordinaatisto ja matriisit}.

\item \label{H-m-6: kiertoja}
Näytä, että seuraavissa $\R^3$:n koordinaattien muunnoskaavoissa on kyse koordinaatiston
kierrosta. Mikä on käänteinen muunnoskaava? Minkä pisteiden koordinaatit pysyvät kierrossa
ennallaan?
\begin{align*}
&\text{a)}\ \ \begin{bmatrix} x'\\y'\\z' \end{bmatrix} =
              \frac{1}{7} \begin{rmatrix} 3&-2&6\\2&-6&-3\\6&3&-2 \end{rmatrix}
              \begin{bmatrix} x\\y\\z \end{bmatrix} \\[2mm]
&\text{b)}\ \ \begin{bmatrix} x'\\y'\\z' \end{bmatrix} =
              \frac{1}{15} \begin{rmatrix} 10&-5&10\\-11&-2&10\\-2&-14&-5 \end{rmatrix}
              \begin{bmatrix} x\\y\\z \end{bmatrix}
\end{align*}

\item
Seuraavien matriisien sarakket ja rivit virittävät erään $\R^n$:n aliavaruuden
($n=3,4,5$ tai $7$). Määritä tuetulla Gaussin algoritmilla ko.\ avaruuden dimensio ja jokin 
kanta.
\begin{align*}
&\text{a)}\ \ \begin{rmatrix} 1&5&0\\1&2&3\\1&3&2 \end{rmatrix} \qquad
 \text{b)}\ \ \begin{rmatrix} 3&4&-2&1\\2&1&3&-2\\1&-2&8&-5 \end{rmatrix} \\[3mm]
&\text{c)}\ \ \begin{rmatrix} 
              1&2&-3&1&5\\3&0&3&-3&3\\2&-1&4&-3&1\\0&2&-4&2&4 
              \end{rmatrix} \qquad
 \text{d)}\ \ \begin{rmatrix}
              1&0&-1&-1&3&3&5\\1&1&1&2&2&-1&3\\1&2&3&5&1&-5&1\\3&2&1&3&7&2&11
              \end{rmatrix}
\end{align*}

\item (*)
$\R^3$:n yleinen kierretty (ortonormeerattu ja positiivisesti suunnistettu) kanta
$\{\ma,\mb,\mc\}$ syntyy kiertämällä kantaa $\{\me_1,\me_2,\me_3\}$ $x_1$-akselin ympäri
kulma $\alpha$, kierron tulosta $x_2$-akselin ympäri kulma $\beta$ ja lopuksi näiden kiertojen
tulosta $x_3$-akselin ympäri kulma $\gamma$. Määritä vektorit $\ma,\mb,\mc$ kulmien
$\,\alpha,\beta,\gamma\,$ avulla. Millä kulmien arvoilla syntyy a) pallokoordinaattikanta
$\{\me_r,\me_\theta,\me_\varphi\}$ vastaten pallonpintakoordinaatteja $\,\theta,\varphi\,$,\,
b) tehtävän \ref{H-m-6: kiertoja}a kierretty kanta?

\item (*)
Olkoon $W\subset\R^4$ aliavaruus, jonka virittävät vektorit $\ma=[1,1,1,1]$ ja 
$\mb=[1,1,-1,-1]$. Määritä vektorin $\mpu=[1,2,-1,3]$ ortogonaaliprojektio aliavaruuteen $W$ \
a) euklidisen skalaaritulon, \ b) skalaaritulon $\scp{\mx}{\my}=x_1y_1+x_2y_2+2x_3y_3+3x_4y_4\,$
mielessä. Millaisen minimointiongelman muotoa
\[
\mx \in W:\ f(\mx)\ =\ \text{min!}
\]
nämä ortogonaaliprojektiot ratkaisevat?

\end{enumerate}
